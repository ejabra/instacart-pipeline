# üõí Instacart Real-Time Supply Chain Pipeline

![Status](https://img.shields.io/badge/Status-Completed-success)
![Python](https://img.shields.io/badge/Python-3.9-blue)
![Docker](https://img.shields.io/badge/Docker-Compose-orange)
![ClickHouse](https://img.shields.io/badge/Database-ClickHouse-yellow)
![Kafka](https://img.shields.io/badge/Streaming-Kafka-black)

Un pipeline Big Data de bout en bout pour pr√©dire la demande et optimiser les stocks en temps r√©el, bas√© sur le dataset public **Instacart**.

---

## üöÄ Objectif du Projet
R√©duire le gaspillage alimentaire et √©viter les ruptures de stock gr√¢ce √† une architecture Data Streaming et au Machine Learning.

* **Probl√®me :** Gestion statique des stocks inefficace face √† la volatilit√© de la demande.
* **Solution :** Ingestion temps r√©el et pr√©diction du prochain achat utilisateur.
* **Performance ML :** Mod√®le Random Forest avec un **R¬≤ de 0.79**.

---

## üèóÔ∏è Architecture Technique

Le projet suit un flux ETL/ELT moderne enti√®rement conteneuris√© :

```mermaid
graph LR
A[Ingestion: Apache NiFi] -->|JSON Stream| B(Broker: Kafka)
B -->|Consumer Python| C{Processing & Enrichment}
C -->|Storage| D[(ClickHouse OLAP)]
C -->|Metadata| E[(MySQL)]
D -->|Visualization| F[Streamlit Dashboard]
D -->|BI| G[Power BI]
üõ†Ô∏è Tech Stack
Ingestion : Apache NiFi (Gestion de flux, Idempotence, Backpressure)

Streaming : Apache Kafka & Zookeeper (Message Broker haute performance)

Stockage : * ClickHouse (Big Data Analytics - OLAP)

MySQL (Lookup & M√©tadonn√©es relationnelles)

Processing : Python (Pandas, Kafka-Python, OpenLineage)

Machine Learning : Scikit-learn (Random Forest pour la pr√©diction de demande)

Visualisation : Streamlit (Apps Data Temps R√©el) & Power BI (Analyse historique)

Infrastructure : Docker & Docker Compose

üì¶ Installation & D√©marrage
Pr√©-requis
Docker & Docker Compose

Python 3.9+

Git

1. Cloner le projet
Bash

git clone [https://github.com/votre-username/instacart-pipeline.git](https://github.com/votre-username/instacart-pipeline.git)
cd instacart-pipeline
2. Lancer l'infrastructure (Docker)
Assurez-vous que les ports 8080 (NiFi), 9092 (Kafka), 8123 (ClickHouse) et 3000 (Marquez) sont libres.

Bash

docker-compose up -d
V√©rifiez que les conteneurs sont bien lanc√©s via docker ps.

3. Installer les d√©pendances Python
Il est recommand√© d'utiliser un environnement virtuel.

Bash

pip install -r requirements.txt
4. Lancer le Pipeline
D√©marrer le Consumer (Enrichissement & Stockage) :

Bash

python consumer.py
Lancer le Dashboard ML (Streamlit) :

Bash

streamlit run app.py
üìä Fonctionnalit√©s Cl√©s
‚úÖ Ingestion R√©siliente : Gestion des doublons (Deduplication) et transformation √† la vol√©e via NiFi.

‚úÖ Analytics Temps R√©el : Calcul instantan√© des indicateurs cl√©s (KPIs) via ClickHouse.

‚úÖ Data Lineage : Tra√ßabilit√© des flux de donn√©es (Compatible OpenLineage/Marquez).

‚úÖ Pr√©diction de Stock : Estimation des volumes de commandes par produit et par jour pour la Supply Chain.

üë• Auteurs
Brahim DARGUI - Data Engineer & Architecture
Nouhaila BENNANI - Data Analyst & Machine Learning

Projet de fin de formation - Ynov Campus (2025)